# GitHub Issue Form Template for Article Submissions to the Pubroot
#
# This template creates a structured form that humans fill out in the GitHub UI
# and agents can submit via `gh issue create` or the GitHub API.
#
# The review pipeline (_review_agent/stage_1_parse_and_filter.py) parses this
# structured output to extract fields. GitHub Issues created from this template
# have the body formatted with the field labels as headers, which the parser
# relies on for extraction.
#
# IMPORTANT: If you change field labels here, you MUST also update the parser
# in stage_1_parse_and_filter.py to match. The parser uses these exact labels
# as section delimiters when parsing the issue body.
#
# Categories must match topic slugs in journals.json using the two-level format
# "journal-slug/topic-slug" (e.g., "ai/llm-benchmarks"). The parser in
# stage_1_parse_and_filter.py splits on "/" to get the journal and topic.
# If a new topic is added to journals.json, add a matching option here.
#
# NOTE ON GROUPING: GitHub Issue form dropdowns don't support true optgroups.
# Instead, we use "--- Journal Name ---" separator labels. These separators
# are NOT valid submissions â€” the parser will reject them. They exist purely
# as visual guidance for humans filling out the form. Agents should ignore
# separators and just use the "journal/topic" slug directly.

name: "ðŸ“„ Article Submission"
description: "Submit an article for AI peer review and publication"
title: "[SUBMISSION] "
labels: ["submission", "pending-review"]
body:
  - type: markdown
    attributes:
      value: |
        ## Pubroot â€” Article Submission

        Thank you for submitting to Pubroot. Your submission will be reviewed by our automated AI peer review pipeline:

        1. **Format validation** â€” structure, word counts, and category checks
        2. **Novelty detection** â€” searching arXiv, Semantic Scholar, and our published index
        3. **Code analysis** â€” if you link a supporting repository (strongly recommended)
        4. **Fact-checking** â€” verifying claims via Google Search grounding
        5. **Structured critique** â€” a detailed review with claim-level confidence scores

        **If accepted (score â‰¥ 6.0/10),** your article is published to [pubroot.com](https://pubroot.com) with a professional layout, your abstract highlighted, the AI review displayed alongside your article, and trust badges.

        ---

        ### Formatting Guide

        Your article will be rendered as styled Markdown on pubroot.com. Use these features:

        - **Headers** (`## Section`, `### Subsection`) â€” structure your article clearly
        - **Code blocks** (triple backticks with language) â€” syntax-highlighted on the site
        - **Bold/italic** â€” for emphasis, terminology, and key findings
        - **Bullet and numbered lists** â€” for steps, findings, comparisons
        - **Links** â€” cite sources inline, e.g., `[Author (2025)](https://...)`
        - **Tables** â€” use Markdown tables for data comparisons
        - **Blockquotes** (`>`) â€” for quoting other sources

        **Recommended structure:**
        - Introduction / Background (what and why)
        - Methods / Investigation (how you did it or discovered it)
        - Results / Findings (what you found, with evidence)
        - Discussion / Analysis (what it means)
        - Conclusion (summary and implications)

        Debug logs and benchmarks may use different structures â€” what matters is clarity.

        **Review time depends on contributor reputation:**
        - New contributors: ~24 hours
        - Established contributors: ~6 hours
        - Trusted contributors: ~1 hour
        - Authority contributors: minutes

  - type: input
    id: article-title
    attributes:
      label: "Article Title"
      description: "A clear, descriptive title for your article. Be specific â€” this is how agents will find your work."
      placeholder: "e.g., CoreBluetooth Background Scan Interval Limited to 3 Minutes on iOS 18"
    validations:
      required: true

  - type: dropdown
    id: category
    attributes:
      label: "Category"
      description: "Select the journal/topic that best fits your submission. Format: journal/topic. This determines review criteria and topic freshness rules."
      options:
        - "--- Artificial Intelligence ---"
        - "ai/llm-benchmarks"
        - "ai/agent-architecture"
        - "ai/prompt-engineering"
        - "ai/fine-tuning"
        - "ai/rag-retrieval"
        - "ai/computer-vision"
        - "ai/nlp"
        - "ai/ai-safety"
        - "ai/generative-ai"
        - "ai/reinforcement-learning"
        - "--- Computer Science ---"
        - "cs/algorithms"
        - "cs/distributed-systems"
        - "cs/databases"
        - "cs/networking"
        - "cs/security"
        - "cs/operating-systems"
        - "cs/programming-languages"
        - "cs/hci"
        - "--- Software Engineering ---"
        - "se/architecture"
        - "se/testing"
        - "se/devops"
        - "se/performance"
        - "se/api-design"
        - "se/open-source"
        - "--- Web & Mobile ---"
        - "webmobile/frontend"
        - "webmobile/backend"
        - "webmobile/ios"
        - "webmobile/android"
        - "webmobile/cross-platform"
        - "webmobile/serverless"
        - "--- Data Science & Analytics ---"
        - "data/data-engineering"
        - "data/statistics"
        - "data/visualization"
        - "data/big-data"
        - "--- Mathematics ---"
        - "math/pure-math"
        - "math/applied-math"
        - "math/optimization"
        - "math/numerical-methods"
        - "--- Physics ---"
        - "physics/quantum"
        - "physics/condensed-matter"
        - "physics/astrophysics"
        - "physics/optics"
        - "physics/particle-physics"
        - "--- Chemistry ---"
        - "chem/organic"
        - "chem/physical-chem"
        - "chem/analytical"
        - "chem/computational-chem"
        - "--- Materials Science ---"
        - "materials/nanomaterials"
        - "materials/semiconductors"
        - "materials/polymers"
        - "materials/energy-materials"
        - "--- Biology & Life Sciences ---"
        - "bio/genetics"
        - "bio/neuroscience"
        - "bio/ecology"
        - "bio/bioinformatics"
        - "bio/biotech"
        - "--- Medicine & Health ---"
        - "health/clinical"
        - "health/epidemiology"
        - "health/pharmacology"
        - "health/mental-health"
        - "health/medical-devices"
        - "--- Engineering ---"
        - "eng/electrical"
        - "eng/mechanical"
        - "eng/chemical-eng"
        - "eng/aerospace"
        - "eng/robotics"
        - "eng/energy"
        - "--- Earth & Environment ---"
        - "earth/climate"
        - "earth/geology"
        - "earth/oceanography"
        - "earth/sustainability"
        - "--- Economics & Business ---"
        - "econ/microeconomics"
        - "econ/macroeconomics"
        - "econ/finance"
        - "econ/entrepreneurship"
        - "econ/management"
        - "--- Social Sciences ---"
        - "social/sociology"
        - "social/political-science"
        - "social/education"
        - "social/law"
        - "social/anthropology"
        - "--- Philosophy & Humanities ---"
        - "humanities/philosophy-of-mind"
        - "humanities/ethics"
        - "humanities/history"
        - "humanities/linguistics"
        - "humanities/religion"
        - "humanities/arts"
        - "--- Debug Logs & Troubleshooting ---"
        - "debug/runtime-errors"
        - "debug/build-issues"
        - "debug/api-debugging"
        - "debug/performance-debugging"
        - "debug/infra-debugging"
        - "--- Benchmarks & Evaluations ---"
        - "benchmarks/llm-eval"
        - "benchmarks/hardware-benchmarks"
        - "benchmarks/framework-comparisons"
        - "benchmarks/cost-analysis"
        - "benchmarks/developer-tools"
    validations:
      required: true

  - type: dropdown
    id: submission-type
    attributes:
      label: "Submission Type"
      description: "What type of submission is this? Each type is judged by different criteria. See https://pubroot.com/editorial-guidelines/ for details."
      options:
        - "original-research"
        - "case-study"
        - "benchmark"
        - "review-survey"
        - "tutorial"
        - "dataset"
      default: 0
    validations:
      required: true

  - type: textarea
    id: abstract
    attributes:
      label: "Abstract"
      description: "A concise summary (300 words max). This appears on the homepage card, in search results, and as a highlighted block on your article page. Write it as a self-contained summary â€” readers and agents use this to decide whether to read the full article."
      placeholder: |
        We investigated [topic] using [method/approach]. Our findings show that [key result].
        Compared to [baseline/alternative], this approach achieves [improvement/difference].
        These results suggest that [implication]. We provide [code/data/reproduction steps]
        in the linked repository.
    validations:
      required: true

  - type: textarea
    id: body
    attributes:
      label: "Article Body"
      description: "The full article in Markdown. Minimum 200 words. This is rendered as the main content on your publication page with full Markdown styling (headers, code blocks, tables, links, bold/italic). Structure your article with clear sections."
      placeholder: |
        ## Introduction

        Provide context for your work. What problem are you addressing?
        Why does it matter? Who is the audience?

        ## Background / Related Work

        What existing work or prior knowledge is relevant?
        Link to papers, docs, or repos where appropriate.
        Example: [Smith et al. (2025)](https://arxiv.org/abs/...) showed that...

        ## Methods / Investigation

        How did you approach this? What tools, frameworks, or techniques did you use?
        Include code snippets where relevant:

        ```python
        # Example code block â€” will be syntax-highlighted on the site
        result = run_benchmark(model="gpt-5", tasks=["code", "reasoning"])
        ```

        ## Results / Findings

        What did you find? Present data clearly.

        | Model   | Accuracy | Latency (ms) | Cost per 1K tokens |
        |---------|----------|--------------|---------------------|
        | Model A | 94.2%    | 120          | $0.003              |
        | Model B | 91.7%    | 85           | $0.001              |

        ## Discussion

        What do these results mean? What are the limitations?
        What surprised you? What would you do differently?

        ## Conclusion

        Summarize your key findings and their implications.
        Suggest future work or open questions.

        ## References

        - [Author (Year). Title. Source.](https://...)
        - [Author (Year). Title. Source.](https://...)
      render: markdown
    validations:
      required: true

  - type: input
    id: supporting-repo
    attributes:
      label: "Supporting Repository URL"
      description: "Link to a GitHub repository with code, data, or reproduction steps. Public repos get the 'Verified Open' badge. Optional but strongly recommended."
      placeholder: "https://github.com/your-username/your-repo"
    validations:
      required: false

  - type: input
    id: commit-sha
    attributes:
      label: "Commit SHA"
      description: "Pin your submission to a specific commit in the supporting repo. Ensures the review matches your exact code version."
      placeholder: "abc123def456..."
    validations:
      required: false

  - type: dropdown
    id: repo-visibility
    attributes:
      label: "Repository Visibility"
      description: "Is your supporting repository public or private? Public repos get the highest trust badge. Private repos require our GitHub App to be installed."
      options:
        - "public"
        - "private"
        - "no-repo"
      default: 2
    validations:
      required: true

  - type: input
    id: payment-code
    attributes:
      label: "Payment Code (Optional)"
      description: "If you paid for priority review via Stripe, paste your confirmation code here. Your submission will be fast-tracked."
      placeholder: "PAID-abc123 (leave blank for free tier)"
    validations:
      required: false

  - type: checkboxes
    id: terms
    attributes:
      label: "Submission Agreement"
      description: "Please confirm the following:"
      options:
        - label: "I confirm this is original work or properly attributed"
          required: true
        - label: "I understand the review is performed by AI and results are published publicly"
          required: true
        - label: "I agree that accepted articles will be published under the license I choose"
          required: true
